{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWxRt3SE8LG"
      },
      "source": [
        "###Packages "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRFcLklEE7Bg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ! pip install TextBlob\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "# Text polarity\n",
        "from textblob import TextBlob\n",
        "\n",
        "# For model-building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer,HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "\n",
        "# SVD\n",
        "from sklearn.decomposition import TruncatedSVD,PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Bag of words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# For text pre-processing\n",
        "import re, string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp5UJBkrFYJr"
      },
      "source": [
        "### Data \n",
        "Getting the.   data. First, we are going to get the data from the json. Then we are going to process it. And then use ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U05e6LnXFaR8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Read the json file\n",
        "trainSet = pd.read_json(\"./train.json\")\n",
        "testSet = pd.read_json(\"./test.json\")\n",
        "trainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UNzQelNF1Jr"
      },
      "source": [
        "We are going to remove the papedId and authorName because it won't tell us nothing about the author. \n",
        "(We already have authorId to identify the author, so let's remove authorName too)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvs_eNiiFibk"
      },
      "outputs": [],
      "source": [
        "trainSet = trainSet.drop('authorName',axis = 1)\n",
        "trainSet = trainSet.drop('paperId',axis = 1)\n",
        "testSet = testSet.drop('paperId',axis = 1)\n",
        "\n",
        "# Now, let's have a look to what we have:\n",
        "\n",
        "# We take an example: The amount of data for authorId 3188285\n",
        "\n",
        "print(\"Amount of data:\",len(trainSet.loc[trainSet[\"authorId\"]==3188285]))\n",
        "# First instance:\n",
        "print(trainSet.loc[trainSet[\"authorId\"]==3188285].iloc[1])\n",
        "# All the venues from this author\n",
        "print(trainSet.loc[trainSet[\"authorId\"]==3188285,[\"venue\"]])\n",
        "\n",
        "# So for every author we must somehow encode the title and the abstract. \n",
        "# Furthermore, we might deduce from the previous example that author likes to write about a topic related with venue \"CLPsych\". \n",
        "# Thus, we can consider splitting venues.\n",
        "\n",
        "# Also, we can merge title and abstract. The \"Type of writing\" is the same.\n",
        "\n",
        "# Merge title and abstract\n",
        "trainSet[\"text\"] = trainSet[\"title\"] + '. '+ trainSet[\"abstract\"]\n",
        "trainSet = trainSet.drop('abstract',axis = 1)\n",
        "trainSet = trainSet.drop('title',axis = 1)\n",
        "# Same in test dataset\n",
        "testSet[\"text\"] = testSet[\"title\"] + '. '+ testSet[\"abstract\"]\n",
        "testSet = testSet.drop('abstract',axis = 1)\n",
        "testSet = testSet.drop('title',axis = 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Split venues\n",
        "def splitVenues(dataSet):\n",
        "  dataSet[\"venue\"] = (dataSet[\"venue\"].str.replace(\"2\\d{3}\",\"\", regex = True)).str.strip();\n",
        "  arr = []\n",
        "  for i in dataSet[\"venue\"]:\n",
        "      if i.find('@') == -1:\n",
        "          arr.append([\"\",i])\n",
        "      else:\n",
        "          arr.append(i.split(\"@\"))\n",
        "        \n",
        "  dataSet = pd.concat([dataSet, pd.DataFrame(arr,columns = [\"campus\",\"uni\"])],axis = 1).drop(\"venue\",axis = 1)\n",
        "  return dataSet\n",
        "\n",
        "trainSet = splitVenues(trainSet)\n",
        "testSet = splitVenues(testSet)\n",
        "trainSet.head(3)\n",
        "\n",
        "# Now let's keep the objective that we are trying to predict aside.\n",
        "\n",
        "# We are going to try and predict the authorId\n",
        "goal = trainSet[\"authorId\"]\n",
        "\n",
        "#Lets remove also the id\n",
        "trainSet = trainSet.drop('authorId',axis = 1)\n",
        "\n",
        "# Finally, before we start processing text, we might want to split the years also:\n",
        "\n",
        "# Fine, so to start processing the text we are going to obtain the lemmas, for example, having -> have.\n",
        "\n",
        "# Credits https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e\n",
        "\n",
        "# Convert to lowercase, strip and remove punctuations\n",
        "def preprocess(text):\n",
        "    text = text.lower() \n",
        "    text=text.strip()  \n",
        "    text=re.compile('<.*?>').sub('', text) \n",
        "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
        "    text = re.sub('\\s+', ' ', text)  \n",
        "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
        "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
        "    text = re.sub(r'\\d',' ',text) \n",
        "    text = re.sub(r'\\s+',' ',text) \n",
        "    return text\n",
        " \n",
        "# STOPWORD REMOVAL(deleting neutral words such as and)\n",
        "def stopword(string):\n",
        "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
        "    return ' '.join(a)\n",
        "\n",
        "# LEMMATIZATION (to obtain words lemmas)\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        " \n",
        "# This is a helper function to map NTLK position tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Tokenize the sentence\n",
        "def lemmatizer(string):\n",
        "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
        "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
        "    return \" \".join(a)\n",
        "\n",
        "# Here we just preprocess the strings, removed the stopwords and lemmatized them. You can find more about this in:\n",
        "# \n",
        "# *   https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained\n",
        "# *   https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e\n",
        "# \n",
        "\n",
        "def finalpreprocess(string):\n",
        "    return lemmatizer(stopword(preprocess(string)))\n",
        "trainSet['clean_text'] = trainSet['text'].apply(lambda x: finalpreprocess(x))\n",
        "trainSet = trainSet.drop('text', axis = 1)\n",
        "\n",
        "def finalpreprocess(string):\n",
        "    return lemmatizer(stopword(preprocess(string)))\n",
        "testSet['clean_text'] = testSet['text'].apply(lambda x: finalpreprocess(x))\n",
        "testSet = testSet.drop('text', axis = 1)\n",
        "\n",
        "# We are going to save this file, because getting the lemmas is a heavy process.\n",
        "\n",
        "testSet.head()\n",
        "\n",
        "# Let's count the words in every text, unique words, characters, stop_words, average word length\n",
        "# punctiation, text polarity, subjectivity...\n",
        "\n",
        "def useful_text_features(og_train):\n",
        "    df = pd.DataFrame()\n",
        "    df[\"num_words\"] = (og_train[\"title\"]+\". \"+og_train[\"abstract\"]).apply(lambda x: len(str(x).split()))\n",
        "    df[\"num_uniq_words\"] = (og_train[\"title\"]+\". \"+og_train[\"abstract\"]).apply(lambda x: len(set(str(x).split())))\n",
        "    df[\"chars\"] = (og_train[\"title\"]+\". \"+og_train[\"abstract\"]).apply(lambda x: len(str(x)))\n",
        "    df[\"stop_words\"] = (og_train[\"title\"]+\". \"+og_train[\"abstract\"]).apply(\n",
        "      lambda x: len([w for w in str(x).lower().split() if w in stopwords.words(\"english\")]))\n",
        "\n",
        "    df[\"num_punc\"] = (og_train[\"title\"]+\". \"+og_train[\"abstract\"]).apply(\n",
        "      lambda x: len([w for w in str(x)  if w in list(string.punctuation)]))\n",
        "\n",
        "    df[\"avg_word_len\"] = (og_train[\"title\"]+\". \"+og_train[\"abstract\"]).apply(\n",
        "      lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "    df[\"text_polarity\"] = (og_train[\"title\"]+\". \"+og_train[\"abstract\"]).apply(\n",
        "      lambda x: TextBlob(x).sentiment[0])\n",
        "    df[\"text_subj\"] = (og_train[\"title\"]+\". \"+og_train[\"abstract\"]).apply(\n",
        "      lambda x: TextBlob(x).sentiment[1])\n",
        "    return df\n",
        "\n",
        "og_train = pd.read_json(\"./train.json\")\n",
        "og_test = pd.read_json(\"./test.json\")\n",
        "\n",
        "trainSet[[\"num_words\", \"num_uniq_words\", \"chars\", \"stop_words\", \"num_punc\", \"avg_word_len\",\"text_polarity\",\"text_subj\"]] = useful_text_features(og_train)[[\"num_words\", \"num_uniq_words\", \"chars\", \"stop_words\", \"num_punc\", \"avg_word_len\",\"text_polarity\",\"text_subj\"]]\n",
        "   \n",
        "testSet[[\"num_words\", \"num_uniq_words\", \"chars\", \"stop_words\", \"num_punc\", \"avg_word_len\",\"text_polarity\",\"text_subj\"]] = useful_text_features(og_test)[[\"num_words\", \"num_uniq_words\", \"chars\", \"stop_words\", \"num_punc\", \"avg_word_len\",\"text_polarity\",\"text_subj\"]]      \n",
        "\n",
        "\n",
        "# We're going to save the lemmatized document, to run code faster next time\n",
        "trainSet.to_json('data_lemmatized.json')\n",
        "testSet.to_json('test_data_lemmatized.json')\n",
        "\n",
        "# Focus on the words:\n",
        "\n",
        "# So after we removed the useless words, our approach is to get the common words \n",
        "# among the different author publications and also the test dataSet.\n",
        "# Hopefully this will help sweeping up the useless words \n",
        "# that don't really gather the author's speech intention.\n",
        "\n",
        "trainSet = pd.read_json(\"./data_lemmatized.json\") #c\n",
        "og_train = pd.read_json(\"./train.json\") #c\n",
        "testSet = pd.read_json(\"./test_data_lemmatized.json\") #c\n",
        "trainSet = pd.concat([trainSet, og_train[[\"authorId\",\"paperId\"]]], axis = 1)\n",
        "\n",
        "# We tokenize words:\n",
        "\n",
        "test_word_set = set()\n",
        "for i in testSet[\"clean_text\"]:\n",
        "    test_word_set.update(set(nltk.word_tokenize(i) ))\n",
        "1692242\n",
        "\n",
        "new_text = []\n",
        "\n",
        "for index, row in trainSet.iterrows():\n",
        "    other_val = trainSet[lambda x: (x[\"authorId\"] == row[\"authorId\"]) & (x[\"paperId\"] != row[\"paperId\"])]\n",
        "    if(other_val.shape == 0):\n",
        "        new_text.append(row[\"clean_text\"])\n",
        "        continue\n",
        "    tokens = nltk.word_tokenize(row[\"clean_text\"])\n",
        "    valores = list()\n",
        "    for token in tokens:\n",
        "        if(token in test_word_set):\n",
        "            valores.append(token)\n",
        "        else:\n",
        "            for index, value in other_val[\"clean_text\"].items():\n",
        "                if value.find(token)>=0:\n",
        "                    valores.append(token)\n",
        "    new_text.append(\" \".join(valores))\n",
        "    #print(valores,row[\"clean_text\"])\n",
        "\n",
        "trainSet[\"clean_text\"] = new_text\n",
        "nuevo_text = None\n",
        "\n",
        "trainSet.to_json(\"common_words_dataset.json\")\n",
        "\n",
        "# Analyzing the text(encoding all the text features for computer to understand):\n",
        "\n",
        "# We are going to start back from the saved data\n",
        "trainSet = pd.read_json(\"./common_words_dataset.json\")\n",
        "# og_train = pd.read_json(\"./Desktop/EjercicioML/train.json\")\n",
        "# testSet = pd.read_json(\"./Desktop/EjercicioML/test_data_lemmatized.json\")\n",
        "goal = trainSet[\"authorId\"]\n",
        "trainSet.head(3)\n",
        "\n",
        "# dataSet = dataSet.drop(\"campus\",axis = 1)\n",
        "trainSet = trainSet.drop(\"authorId\",axis = 1 )\n",
        "trainSet = trainSet.drop(\"paperId\",axis = 1 )\n",
        "\n",
        "# We used these \"few\" lines just to work with less data willing to improve speed:\n",
        "data_new = trainSet\n",
        "new_goal = goal\n",
        "trainSet = None\n",
        "goal = None\n",
        "\n",
        "# This counts the number of nouns, pronouns, verbs, adjectives...\n",
        "def pos_count(sent):\n",
        "    nn_count = 0   #Noun\n",
        "    pr_count = 0   #Pronoun\n",
        "    vb_count = 0   #Verb\n",
        "    jj_count = 0   #Adjective\n",
        "    uh_count = 0   #Interjection\n",
        "    cd_count = 0   #Numerics\n",
        "    \n",
        "    sent = nltk.word_tokenize(sent)\n",
        "    sent = nltk.pos_tag(sent)\n",
        "    for token in sent:\n",
        "        if token[1] in ['NN','NNP','NNS']:\n",
        "                    nn_count += 1\n",
        "        if token[1] in ['PRP','PRP$']:\n",
        "                    pr_count += 1\n",
        "        if token[1] in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n",
        "                    vb_count += 1\n",
        "        if token[1] in ['JJ','JJR','JJS']:\n",
        "                    jj_count += 1\n",
        "        if token[1] in ['UH']:\n",
        "                    uh_count += 1\n",
        "        if token[1] in ['CD']:\n",
        "                    cd_count += 1\n",
        "    \n",
        "    return pd.Series([nn_count, pr_count, vb_count, jj_count, uh_count, cd_count])\n",
        "\n",
        "\n",
        "data_new[[\"nn_count\", \"pr_count\", \"vb_count\", \"jj_count\", \"uh_count\", \"cd_count\"]] = data_new[\"clean_text\"].apply(pos_count)\n",
        "\n",
        "# Credits:\n",
        "# *   https://betterprogramming.pub/beginners-to-advanced-feature-engineering-from-text-data-c228047a4813\n",
        "\n",
        "# We need to transform campus, uni to float\n",
        "enc = LabelEncoder()\n",
        "enc.fit(list(data_new[\"campus\"])+list(data_new[\"uni\"]))\n",
        "data_new[\"campus\"] = enc.transform(data_new[\"campus\"])\n",
        "data_new[\"uni\"] = enc.transform(data_new[\"uni\"])\n",
        "\n",
        "# We are going to count the number of appearances of each word\n",
        "# Tfidf doesn't improve accuracy, but maybe we should try others\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(data_new[\"clean_text\"])\n",
        "X = pd.DataFrame(X.toarray())\n",
        "\n",
        "# X.columns = cv.get_feature_names_out()\n",
        "X.index = data_new.index\n",
        "data_new = pd.concat([data_new,X],axis = 1).drop(\"clean_text\",axis = 1)\n",
        "X = None\n",
        "data_new.head(2)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(data_new, new_goal, test_size=0.1, random_state = 42)\n",
        "x_train.head()\n",
        "\n",
        "# Freeing Memory\n",
        "data_new = None\n",
        "new_goal = None\n",
        "\n",
        "\"\"\"clf = RandomForestClassifier( verbose=1,n_jobs = 4)\n",
        "clf.fit(x_train, y_train)\n",
        "print(\"Accuracy of Model :\", clf.score(x_val, y_val))\n",
        "\n",
        "# We are going to optimize the RFC:\n",
        "\n",
        "# Number of trees in random forest (trying to implement hyperparameter tunning for random forest )\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1500, num = 7)]\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(50, 110, num = 4)]\n",
        "#max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10, 15]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1,3,5]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True]\n",
        "# Create the random grid\n",
        "random_grid = {'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf}\n",
        "print(random_grid)\n",
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestClassifier(n_jobs = 3)\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# Search across 100 different combinations, and use all available cores\n",
        "rf_random = GridSearchCV(estimator = rf, param_grid = random_grid,cv=2, verbose=3, n_jobs = 1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(x_train.values, y_train.values) \"\"\"\n",
        "\n",
        "\"\"\"print(\"\\n The best estimator across ALL searched params:\\n\", rf_random.best_estimator_)\n",
        "print(\"\\n The best score across ALL searched params:\\n\", rf_random.best_score_)\n",
        "print(\"\\n The best parameters across ALL searched params:\\n\", rf_random.best_params_)\"\"\"\n",
        "\n",
        "mult=MultinomialNB()\n",
        "mult.fit(x_train, y_train)\n",
        "alp ={'alpha':(1,0.1,0.01,0.05,0.001,0.005,0.0001,0.0005)}\n",
        "#hyperpara\n",
        "grid_mult=GridSearchCV(mult, alp,n_jobs=-1)\n",
        "grid_mult.fit(x_train, y_train)\n",
        "\n",
        "print(grid_mult.score(x_val, y_val))\n",
        "\n",
        "# Final train + writing the solution\n",
        "\n",
        "# We start off with the processed data from the first section:\n",
        "\n",
        "#Let's train the model again\n",
        "trainSet = pd.read_json(\"./common_words_dataset.json\")\n",
        "testSet = pd.read_json(\"./test_data_lemmatized.json\")\n",
        "\n",
        "trainSet = trainSet.drop(\"paperId\",axis=1)\n",
        "#testSet = splitVenues(testSet)\n",
        "y = trainSet[\"authorId\"]\n",
        "trainSet = trainSet.drop(\"authorId\",axis=1)\n",
        "trainSet.head(1)\n",
        "\n",
        "trainSet[[\"nn_count\", \"pr_count\", \"vb_count\", \"jj_count\", \"uh_count\", \"cd_count\"]] = trainSet[\"clean_text\"].apply(pos_count)\n",
        "testSet[[\"nn_count\", \"pr_count\", \"vb_count\", \"jj_count\", \"uh_count\", \"cd_count\"]] = testSet[\"clean_text\"].apply(pos_count)\n",
        "\n",
        "#transform it to floats\n",
        "enc = LabelEncoder()\n",
        "enc.fit(list(trainSet[\"campus\"])+list(trainSet[\"uni\"])+list(testSet[\"campus\"])+list(testSet[\"uni\"]))\n",
        "trainSet[\"campus\"] = enc.transform(trainSet[\"campus\"])\n",
        "trainSet[\"uni\"] = enc.transform(trainSet[\"uni\"])\n",
        "testSet[\"campus\"] = enc.transform(testSet[\"campus\"])\n",
        "testSet[\"uni\"] = enc.transform(testSet[\"uni\"])\n",
        "\n",
        "# We are going to count the number of appearances of each word\n",
        "# Tfidf doesn't improve accuracy, but maybe we should try others\n",
        "cv = CountVectorizer()\n",
        "# We first fit all values\n",
        "cv.fit(list(trainSet[\"clean_text\"])+list(testSet[\"clean_text\"]))\n",
        "# And then transform just the data/test\n",
        "X = cv.transform(trainSet[\"clean_text\"])\n",
        "X = pd.DataFrame(X.toarray())\n",
        "# X.columns = cv.get_feature_names_out()\n",
        "X.index = trainSet.index\n",
        "trainSet = pd.concat([trainSet,X],axis = 1).drop(\"clean_text\",axis = 1)\n",
        "\n",
        "X = cv.transform(testSet[\"clean_text\"])\n",
        "X = pd.DataFrame(X.toarray())\n",
        "# X.columns = cv.get_feature_names_out()\n",
        "X.index = testSet.index\n",
        "testSet = pd.concat([testSet,X],axis = 1).drop(\"clean_text\",axis = 1)\n",
        "X = None\n",
        "trainSet.head(2)\n",
        "\n",
        "#selected Multinomial \n",
        "mlt=MultinomialNB()\n",
        "mlt.fit(trainSet, y)\n",
        "alp ={'alpha':(1,0.1,0.01,0.05,0.001,0.005,0.0001,0.0005)}\n",
        "grid_mlt=GridSearchCV(mlt, alp)\n",
        "grid_mlt.fit(trainSet, y)\n",
        "\n",
        "# Predictions (generating and writing them in the right format)\n",
        "predictions = grid_mlt.predict(testSet)\n",
        "str(predictions)\n",
        "\n",
        "predictions = pd.DataFrame([str(x) for x in predictions], columns = [\"authorId\"])\n",
        "\n",
        "pd.concat([og_test[\"paperId\"], predictions ],axis = 1).to_json(\"pruebadoc.json\",orient='records', indent=4 )\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
