{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Packages "
      ],
      "metadata": {
        "id": "5sWxRt3SE8LG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRFcLklEE7Bg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ! pip install TextBlob\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "# Text polarity\n",
        "from textblob import TextBlob\n",
        "\n",
        "# For model-building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer,HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "\n",
        "# SVD\n",
        "from sklearn.decomposition import TruncatedSVD,PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Bag of words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# For text pre-processing\n",
        "import re, string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data \n",
        "Getting the.   data. First, we are going to get the data from the json. Then we are going to process it. And then use ML."
      ],
      "metadata": {
        "id": "Rp5UJBkrFYJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read the json file\n",
        "trainSet = pd.read_json(\"./train.json\")\n",
        "testSet = pd.read_json(\"./test.json\")\n",
        "trainSet.head(3)"
      ],
      "metadata": {
        "id": "U05e6LnXFaR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to remove the papedId and authorName because it won't tell us nothing about the author. \n",
        "(We already have authorId to identify the author, so let's remove authorName too)."
      ],
      "metadata": {
        "id": "9UNzQelNF1Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainSet = trainSet.drop('authorName',axis = 1)\n",
        "trainSet = trainSet.drop('paperId',axis = 1)\n",
        "testSet = testSet.drop('paperId',axis = 1)\n",
        "\n",
        "# Now, let's have a look to what we have:\n",
        "\n",
        "# We take an example: The amount of data for authorId 3188285\n",
        "\n",
        "print(\"Amount of data:\",len(trainSet.loc[trainSet[\"authorId\"]==3188285]))\n",
        "# First instance:\n",
        "print(trainSet.loc[trainSet[\"authorId\"]==3188285].iloc[1])\n",
        "# All the venues from this author\n",
        "print(trainSet.loc[trainSet[\"authorId\"]==3188285,[\"venue\"]])\n",
        "\n",
        "# So for every author we must somehow encode the title and the abstract. \n",
        "# Furthermore, we might deduce from the previous example that author likes to write about a topic related with venue \"CLPsych\". \n",
        "# Thus, we can consider splitting venues.\n",
        "\n",
        "# Also, we can merge title and abstract. The \"Type of writing\" is the same.\n",
        "\n",
        "# Merge title and abstract\n",
        "trainSet[\"text\"] = trainSet[\"title\"] + '. '+ trainSet[\"abstract\"]\n",
        "trainSet = trainSet.drop('abstract',axis = 1)\n",
        "trainSet = trainSet.drop('title',axis = 1)\n",
        "# Same in test dataset\n",
        "testSet[\"text\"] = testSet[\"title\"] + '. '+ testSet[\"abstract\"]\n",
        "testSet = testSet.drop('abstract',axis = 1)\n",
        "testSet = testSet.drop('title',axis = 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qvs_eNiiFibk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
